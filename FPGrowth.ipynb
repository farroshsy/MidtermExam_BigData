{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPOUEfa7jpmK0Kg6hOBaBiW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farroshsy/MidtermExam_BigData/blob/main/FPGrowth.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Midterm Exam**\n",
        "## Spark Streaming and Reccomendation Systems\n",
        "---\n",
        "\n",
        "Name: Farros Hilmi Syafei \n",
        "<br>\n",
        "Student ID: 5025201012\n",
        "<br>\n",
        "Class: Big Data A\n",
        "<br>\n",
        "Lecturer: Abdul Munif, S.Kom., M.Sc.\n"
      ],
      "metadata": {
        "id": "vdADvMc7jy0M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem\n",
        "### 2. Recommendation Systems - Frequent Pattern Mining\n",
        "You are given the market basket dataset (market-basket.csv). \n",
        "*   Use the FP-growth algorithm in Apache Spark to find the most frequent items. Save\n",
        "your result into an .xlsx files.\n",
        "*   Do the experiment with different minSupport and minConfidence values. Give the conclusion at the end of experiment.\n",
        "\n"
      ],
      "metadata": {
        "id": "GWTTK_7LRvrD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Driven Dealings Development\n",
        "\n",
        "- EDA on Sales Data\n",
        "- RFM Clustering\n",
        "- Predicting Sales\n",
        "- Market Basket Analysis\n",
        "- Recommending Items per Customer"
      ],
      "metadata": {
        "id": "HRqwGaThj2HA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Market Basket Analysis"
      ],
      "metadata": {
        "id": "lXKGBFzQj_xM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!sudo apt-get update --fix-missing\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "#!wget -q https://downloads.apache.org/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "!mv spark-3.0.0-bin-hadoop3.2.tgz sparkkk\n",
        "!tar xf sparkkk\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "j0a4dTYNlbOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOyyZmreleEC",
        "outputId": "57243b55-c2d7-489a-ddc0-e04b21170f2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spark in /usr/local/lib/python3.9/dist-packages (0.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sets up the environment variables for Java and Spark, initializes the findspark library\n",
        "# Creates a new SparkSession for a PySpark application named 'fpgrowth'.\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('fpgrowth') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "id": "BOWRhLbDliRL",
        "outputId": "3410fbe2-59cb-47c2-a9df-cdcbcc272bf2"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:46625)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
            "    connection = self.deque.pop()\n",
            "IndexError: pop from an empty deque\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
            "    self.socket.connect((self.address, self.port))\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Py4JNetworkError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/content/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m/content/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1114\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1115\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1116\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-3d4531bf511a>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local[*]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-bin-hadoop3.2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    189\u001b[0m                     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msessionState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetConfString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/content/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1029\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m         \"\"\"\n\u001b[0;32m-> 1031\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    977\u001b[0m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_create_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    983\u001b[0m         connection = GatewayConnection(\n\u001b[1;32m    984\u001b[0m             self.gateway_parameters, self.gateway_property)\n\u001b[0;32m--> 985\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1125\u001b[0m                 \u001b[0;34m\"server ({0}:{1})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1127\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_authenticate_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JNetworkError\u001b[0m: An error occurred while trying to connect to the Java server (127.0.0.1:46625)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkLnsawChkaC"
      },
      "outputs": [],
      "source": [
        "# To be able to use your data stored in your Google Drive you first need to mount your Google Drive so you can load and save files to it. \n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "#You'll need to put in a token which Google will generate for you as soon as you click on the link"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "csv_path = '/content/gdrive/MyDrive/BigData/market-basket.csv'\n",
        "with open(csv_path, 'r') as f:\n",
        "    temp_lines = f.readline() + '\\n' + f.readline()\n",
        "    # The csv.Sniffer() method is used to detect the delimiter used in the file (either ; or ,) and then the dialect object is used to read the file with the correct delimiter. The pandas.read_csv() method is used to load the data into a pandas DataFrame object called data.\n",
        "    dialect = csv.Sniffer().sniff(temp_lines, delimiters=';,')\n",
        "    f.seek(0)\n",
        "    data = pd.read_csv(f, dialect=dialect, error_bad_lines=False)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "V9mdjOS6j9CQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.ml.fpm import FPGrowth\n",
        "\n",
        "data = data.astype(str)\n",
        "data['BillNo'] = data['BillNo'].astype(str)\n",
        "\n",
        "sparkdata = spark.createDataFrame(data)\n",
        "basketdata = sparkdata.dropDuplicates(['BillNo', 'Itemname']).sort('BillNo')\n",
        "basketdata = basketdata.groupBy(\"BillNo\").agg(F.collect_list(\"Itemname\")).sort('BillNo')"
      ],
      "metadata": {
        "id": "VRLNIHiKlkk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "29V7BCzvpCCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h1e_b7KKpGCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### First Experiment\n",
        "minSupport = 0.002 and minConfidence == 0.002"
      ],
      "metadata": {
        "id": "Tq3IHvVFhSzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Frequent Pattern Growth – FP Growth is a method of mining frequent itemsets\n",
        "fpGrowth = FPGrowth(itemsCol=\"collect_list(Itemname)\", minSupport=0.002, minConfidence=0.002) \n",
        "model = fpGrowth.fit(basketdata)\n",
        "\n",
        "# Display frequent itemsets.\n",
        "model.freqItemsets.show()\n",
        "items = model.freqItemsets\n",
        "# Display generated association rules.\n",
        "model.associationRules.show()\n",
        "rules = model.associationRules\n",
        "# transform examines the input items against all the association rules and summarize the\n",
        "# consequents as prediction\n",
        "model.transform(basketdata).show()\n",
        "transformed = model.transform(basketdata)"
      ],
      "metadata": {
        "id": "y0WTb42nll8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the Spark DataFrame back to a Pandas DataFrame using Arrow\n",
        "result_pdf = items.select(\"*\").toPandas()\n",
        "result_pdf.head()"
      ],
      "metadata": {
        "id": "p9pXFXjslova"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_pdf.to_excel('result_pdfItemsFreq_Exp1.xlsx')"
      ],
      "metadata": {
        "id": "utO5Rearlqfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rules_pdf = rules.select(\"*\").toPandas()\n",
        "rules_pdf.head()"
      ],
      "metadata": {
        "id": "4VSDz7nwlr4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rules_pdf.to_excel('rules_pdfAnteConseConfLift_Exp1.xlsx')"
      ],
      "metadata": {
        "id": "JG1fWuqgltkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_pdf = transformed.select(\"*\").toPandas()\n",
        "transformed_pdf.head()"
      ],
      "metadata": {
        "id": "krHRCewFlu6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_pdf.to_excel('transformed_pdfSalesTransactionIDCollectListPred_Exp1.xlsx')"
      ],
      "metadata": {
        "id": "Yz5xiXbVlw0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Second Experiment\n",
        "minSupport = 0.005 and minConfidence == 0.005"
      ],
      "metadata": {
        "id": "r51MfQZwiGnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Frequent Pattern Growth – FP Growth is a method of mining frequent itemsets\n",
        "fpGrowth = FPGrowth(itemsCol=\"collect_list(Itemname)\", minSupport=0.005, minConfidence=0.005) \n",
        "model = fpGrowth.fit(basketdata)\n",
        "\n",
        "# Display frequent itemsets.\n",
        "model.freqItemsets.show()\n",
        "items = model.freqItemsets\n",
        "# Display generated association rules.\n",
        "model.associationRules.show()\n",
        "rules = model.associationRules\n",
        "# transform examines the input items against all the association rules and summarize the\n",
        "# consequents as prediction\n",
        "model.transform(basketdata).show()\n",
        "transformed = model.transform(basketdata)"
      ],
      "metadata": {
        "id": "V75VLQZbiJ3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the Spark DataFrame back to a Pandas DataFrame using Arrow\n",
        "result_pdf = items.select(\"*\").toPandas()\n",
        "result_pdf.head()"
      ],
      "metadata": {
        "id": "ZWNmBGVQiL4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_pdf.to_excel('result_pdfItemsFreq_Exp2.xlsx')"
      ],
      "metadata": {
        "id": "fApC45psiOOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rules_pdf = rules.select(\"*\").toPandas()\n",
        "rules_pdf.head()"
      ],
      "metadata": {
        "id": "Sm3qf-R0iRGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rules_pdf.to_excel('rules_pdfAnteConseConfLift_Exp1.xlsx')"
      ],
      "metadata": {
        "id": "ioS3XoeaiTMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_pdf = transformed.select(\"*\").toPandas()\n",
        "transformed_pdf.head()"
      ],
      "metadata": {
        "id": "jt4_mgF1iWiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_pdf.to_excel('transformed_pdfSalesTransactionIDCollectListPred_Exp1.xlsx')"
      ],
      "metadata": {
        "id": "t6-oGm_-iYEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Third Experiment\n",
        "minSupport = 0.01 and minConfidence == 0.01"
      ],
      "metadata": {
        "id": "k8yYDUvtibBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Frequent Pattern Growth – FP Growth is a method of mining frequent itemsets\n",
        "fpGrowth = FPGrowth(itemsCol=\"collect_list(Itemname)\", minSupport=0.01, minConfidence=0.01) \n",
        "model = fpGrowth.fit(basketdata)\n",
        "\n",
        "# Display frequent itemsets.\n",
        "model.freqItemsets.show()\n",
        "items = model.freqItemsets\n",
        "# Display generated association rules.\n",
        "model.associationRules.show()\n",
        "rules = model.associationRules\n",
        "# transform examines the input items against all the association rules and summarize the\n",
        "# consequents as prediction\n",
        "model.transform(basketdata).show()\n",
        "transformed = model.transform(basketdata)"
      ],
      "metadata": {
        "id": "0XvI22pJiecI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the Spark DataFrame back to a Pandas DataFrame using Arrow\n",
        "result_pdf = items.select(\"*\").toPandas()\n",
        "result_pdf.head()"
      ],
      "metadata": {
        "id": "BaPXTIsKiqVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_pdf.to_excel('result_pdfItemsFreq_Exp3.xlsx')"
      ],
      "metadata": {
        "id": "FomdPG5KitAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rules_pdf = rules.select(\"*\").toPandas()\n",
        "rules_pdf.head()"
      ],
      "metadata": {
        "id": "JxE4Eqq0iuh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rules_pdf.to_excel('rules_pdfAnteConseConfLift_Exp1.xlsx')"
      ],
      "metadata": {
        "id": "ucQOAmNwiwAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_pdf = transformed.select(\"*\").toPandas()\n",
        "transformed_pdf.head()"
      ],
      "metadata": {
        "id": "4MsZd8Pbixve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_pdf.to_excel('transformed_pdfSalesTransactionIDCollectListPred_Exp1.xlsx')"
      ],
      "metadata": {
        "id": "4ZiQnGg2iyvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ],
      "metadata": {
        "id": "0t9KnDdxizNZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Minimum Support: This parameter determines the minimum frequency that an itemset must have in order to be considered frequent. Itemsets that occur less frequently than the minimum support threshold are pruned, as they are unlikely to be interesting or informative. Increasing the minimum support value leads to fewer frequent itemsets being generated, while decreasing it leads to more frequent itemsets.\n",
        "*   Minimum Confidence: This parameter determines the minimum level of confidence that an association rule must have in order to be considered interesting or informative. Association rules that have less confidence than the minimum confidence threshold are considered uninteresting and are pruned. Increasing the minimum confidence value leads to fewer association rules being generated, while decreasing it leads to more association rules.\n",
        "\n",
        "Sure! In general, the minimum support and minimum confidence parameters in association rule mining algorithms such as FP-growth can have a significant impact on the results generated.\n",
        "\n",
        "Minimum Support: This parameter determines the minimum frequency that an itemset must have in order to be considered frequent. Itemsets that occur less frequently than the minimum support threshold are pruned, as they are unlikely to be interesting or informative. Increasing the minimum support value leads to fewer frequent itemsets being generated, while decreasing it leads to more frequent itemsets.\n",
        "\n",
        "Minimum Confidence: This parameter determines the minimum level of confidence that an association rule must have in order to be considered interesting or informative. Association rules that have less confidence than the minimum confidence threshold are considered uninteresting and are pruned. Increasing the minimum confidence value leads to fewer association rules being generated, while decreasing it leads to more association rules.\n",
        "\n",
        "In general, finding the optimal values for these parameters requires a balance between generating enough frequent itemsets or association rules to be useful and avoiding overfitting or generating too many irrelevant results. This can be achieved through iterative experimentation with different parameter values, as well as domain knowledge and intuition.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZcVUDtQ0j14O"
      }
    }
  ]
}